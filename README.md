# Patient Education RAG Chatbot
[![Ask DeepWiki](https://devin.ai/assets/askdeepwiki.png)](https://deepwiki.com/Sandesh-Thapa/patient-education-RAG)

This project is a sophisticated RAG (Retrieval-Augmented Generation) chatbot designed for patient education. It leverages official WHO (World Health Organization) guidelines to provide accurate and reliable health information. The application features a Python/FastAPI backend for the RAG pipeline and a modern Next.js/React frontend for the user interface.

## Architecture

This is a full-stack application with a clear separation between the frontend and backend services.

*   **Frontend:**
    *   **Framework:** [Next.js](https://nextjs.org/) with [React](https://react.dev/).
    *   **UI Components:** Built with [Shadcn UI](https://ui.shadcn.com/) and styled with [Tailwind CSS](https://tailwindcss.com/).
    *   **Features:** Responsive design, light/dark mode, real-time streaming of chat responses, and chat history management.

*   **Backend (Python/FastAPI):**
    *   **API Framework:** [FastAPI](https://fastapi.tiangolo.com/) for building high-performance APIs.
    *   **LLM:** [Groq](https://groq.com/) API utilizing the Llama 3.1 model for fast generation.
    *   **Vector Database:** [Pinecone](https://www.pinecone.io/) for efficient storage and retrieval of document embeddings.
    *   **Data Persistence:** [MongoDB](https://www.mongodb.com/) to store and manage chat conversations and history.
    *   **RAG Pipeline:** Powered by [LangChain](https://www.langchain.com/) for document loading (`PyPDFDirectoryLoader`) and text splitting.

## Key Features

*   **Retrieval-Augmented Generation (RAG):** Answers are grounded in information retrieved from uploaded WHO guideline documents, ensuring accuracy.
*   **Real-time Interaction:** Streaming API responses provide a fluid, real-time chatbot experience.
*   **Conversation History:** Chat sessions are saved to MongoDB, allowing users to revisit and continue past conversations.
*   **Dynamic Chat Titling:** Each new conversation is automatically given a concise, relevant title generated by the LLM.
*   **Input Safety Guardrails:** An LLM-based guardrail system validates user input to ensure it is relevant to health topics and to block inappropriate or malicious queries.
*   **Intuitive User Interface:** A clean, responsive interface built with Next.js and Shadcn UI, featuring light and dark modes, chat history navigation, and chat deletion.

## Setup and Usage

### Prerequisites

*   Python 3.8+ and Pip
*   Node.js and npm (or yarn/pnpm)
*   Access keys for Pinecone, Groq, and a configured MongoDB Atlas cluster.

### Backend Setup

1.  **Clone the Repository**
    ```bash
    git clone https://github.com/Sandesh-Thapa/patient-education-RAG.git
    cd patient-education-RAG
    ```

2.  **Set Up Virtual Environment**
    Navigate to the backend directory and create a virtual environment:
    ```bash
    cd backend
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure Environment Variables**
    Create a `.env` file by copying the example and fill in your credentials.
    ```bash
    cp .env.example .env
    ```
    Your `.env` file should contain your keys for Pinecone, Groq, and MongoDB:
    ```env
    PINECONE_API_KEY=YOUR_PINECONE_API_KEY
    PINECONE_INDEX_NAME=your-pinecone-index-name
    PINECONE_INDEX_HOST=your-pinecone-index-host
    GROQ_API_KEY=YOUR_GROQ_API_KEY
    MONGODB_USER=your_mongodb_user
    MONGODB_PASSWORD=your_mongodb_password
    MONGODB_CLUSTER=your_mongodb_cluster_identifier
    ```

5.  **Ingest Data**
    Place your PDF documents (e.g., WHO guidelines) into the `backend/rag/data/` directory. Then, run the ingestion script to process the documents and load them into your Pinecone vector store.
    ```bash
    python rag/load_to_vector_store.py
    ```

6.  **Run the API Server**
    ```bash
    uvicorn api.main:app --reload
    ```
    The API will be available at `http://localhost:8000`.

### Frontend Setup

1.  **Navigate to Frontend Directory**
    In a new terminal window, from the root of the project:
    ```bash
    cd frontend
    ```

2.  **Install Dependencies**
    ```bash
    npm install
    ```

3.  **Run the Development Server**
    ```bash
    npm run dev
    ```
    The application will now be accessible at `http://localhost:3000`.

## API Endpoints

The backend provides several endpoints to manage the chat functionality:

| Method | Endpoint                    | Description                                                   |
| :----- | :-------------------------- | :------------------------------------------------------------ |
| `GET`  | `/`                         | Welcome endpoint to check if the API is running.              |
| `POST` | `/new-chat`                 | Initiates a new chat, generates a title, and streams the response. |
| `POST` | `/chat`                     | Handles subsequent messages in an existing conversation.      |
| `GET`  | `/chats`                    | Retrieves a list of all saved chat conversations.             |
| `GET`  | `/chat/{thread_id}`         | Fetches the full message history for a specific chat.         |
| `DELETE`| `/chat/{thread_id}`         | Deletes a chat conversation and its associated messages.      |
